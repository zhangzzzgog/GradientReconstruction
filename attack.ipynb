{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import math\n",
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from datasets import load_dataset\n",
    "from utils import GumbelSoftmax, get_matrices_expansions, get_embeddings\n",
    "from functional import get_top_in_span,adaptive_sigmoid_probs,reverse_text_embeddings\n",
    "torch.manual_seed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(\"Running on %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "net.eval()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 设置填充token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_input(text, max_length=32):\n",
    "    sample = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        # padding=\"max_length\",  # 确保所有序列等长\n",
    "        max_length=32,         # 根据你的句子长度调整\n",
    "        truncation=True\n",
    "    )\n",
    "    # print(\"first_token:\",sample[\"input_ids\"][0][0])\n",
    "    sample[\"labels\"] = sample[\"input_ids\"].clone()\n",
    "    return sample\n",
    "\n",
    "def label_to_onehot(label,pad_to_len=20):\n",
    "    onehot = torch.zeros(pad_to_len, 50257)\n",
    "    onehot_copy = onehot.clone().detach()\n",
    "    onehot_copy.scatter_(1, label, 1)\n",
    "    return onehot_copy\n",
    "\n",
    "def onehot_criterion(x,pred):\n",
    "    # x: (batch_size, 50257)\n",
    "    # pred: (batch_size, 50257)\n",
    "    loss = torch.nn.BCELoss(reduction='none')(x, pred)\n",
    "    return loss.sum(dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The muscles are your body's \\\"grand central station.\\\"\"  # 替换为你的句子\n",
    "sample = text_to_input(text)\n",
    "sample_len = sample[\"input_ids\"].shape[1]\n",
    "\n",
    "gumbels = [GumbelSoftmax() for _ in range(sample_len)]\n",
    "Us = [nn.Parameter(torch.randn(1, 50257).uniform_(-0.5, 0.5)) for _ in range(sample_len)]\n",
    "\n",
    "\n",
    "for i,m in net.named_parameters():\n",
    "    if \"weight\" in i:\n",
    "        m.data.uniform_(-0.5, 0.5)\n",
    "    if \"bias\" in i:\n",
    "        m.data.uniform_(-0.5, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = onehot_criterion\n",
    "######### honest partipant #########\n",
    "# compute original gradient \n",
    "out = net(**sample)\n",
    "logits =[adaptive_sigmoid_probs(logit) for logit in out.logits.squeeze(0)]\n",
    "logits = torch.stack(logits, dim=0)\n",
    "orig_loss = criterion(logits, label_to_onehot(sample[\"labels\"]))\n",
    "dy_dx = torch.autograd.grad(orig_loss, net.parameters(), create_graph=True, allow_unused=True)\n",
    "\n",
    "# share the gradients with other clients\n",
    "original_dy_dx = list((_.detach().clone() for _ in dy_dx if _ is not None))\n",
    "    \n",
    "dummy_text = \"Any Dummy Text To Be Replaced initially\" \n",
    "dummy_sample = text_to_input(dummy_text)\n",
    "dummy_ids = dummy_sample[\"input_ids\"]\n",
    "dummy_label = dummy_sample[\"labels\"]\n",
    "dummy_label = label_to_onehot(dummy_label, pad_to_len=sample_len)\n",
    "\n",
    "# optimizer = torch.optim.LBFGS(Us,lr=1e-2)\n",
    "optimizer = torch.optim.AdamW(Us,lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "_, R_Qs = get_matrices_expansions(original_dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "for iters in range(300):\n",
    "    def closure():\n",
    "        global dummy_label, dummy_ids\n",
    "        optimizer.zero_grad()  \n",
    "        dummy_logits = torch.zeros_like(dummy_label).to(device)\n",
    "        for pos in range(sample_len):\n",
    "            emb = get_embeddings(net, pos)\n",
    "            probs = get_top_in_span(R_Qs[0], emb, 0.005,'l2')\n",
    "            # gumbel_probs = gumbels[pos](pos_probs[pos],Us[pos],hard=False)\n",
    "            gumbel_probs = gumbels[pos](probs,Us[pos],hard=False)\n",
    "            dummy_logits[pos]= gumbel_probs\n",
    "        \n",
    "        loss = criterion(dummy_logits,dummy_label) \n",
    "        dummy_dy_dx = torch.autograd.grad(loss, net.parameters(), create_graph=True,allow_unused=True)\n",
    "        dummy_dy_dx = (_ for _ in dummy_dy_dx if _ is not None)\n",
    "        grad_diff = 0\n",
    "        grad_count = 0\n",
    "        for gx, gy in zip(dummy_dy_dx, original_dy_dx): # TODO: fix the variablas here\n",
    "            grad_diff += ((gx - gy) ** 2).sum()\n",
    "            grad_count += gx.nelement()\n",
    "        \n",
    "        dummy_ids = torch.argmax(dummy_logits.detach(), dim=-1).unsqueeze(0)\n",
    "        dummy_label = label_to_onehot(dummy_ids, pad_to_len=sample_len)\n",
    "        grad_diff.backward()\n",
    "        return grad_diff\n",
    "\n",
    "    # optimizer.step(closure)\n",
    "    current_loss = closure()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(f\"[Iter {iters}] Current loss:\", \"%.4f\" % current_loss.item())\n",
    "    dummy_text = tokenizer.decode(dummy_ids[0])\n",
    "    print(\"dummy_text:\",dummy_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
